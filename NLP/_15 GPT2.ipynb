{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cyRyTv3BZHahrjmbvnNW0tZkFDT0lM-G","timestamp":1711894433581}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zBjBNQX8kQfh"},"source":["# GPT(Generative Pre-trained Transformer) 2\n","\n","* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"]},{"cell_type":"markdown","metadata":{"id":"gKeqNH_dkTmT"},"source":["* OpenAI에서 GPT 모델 제안\n","* 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝\n","* BERT와 마찬가지로 트랜스포머 모델이지만, BERT는 트랜스포머의 인코더 구조만 사용하고, GPT는 트랜스포머의 디코더 구조(순방향 어텐션)만 사용\n","\n","* GPT2는 GPT1에서 개선되어 레이어 정규화가 부분 블록의 입력쪽에서 사용되고, 셀프 어텐션 이후에 레이어 정규화 적용\n","* GPT2는 GPT1에 비교해 크기가 매우 커진 향상된 모델 사용"]},{"cell_type":"markdown","metadata":{"id":"sDCr0YqjbfLJ"},"source":["## 라이브러리"]},{"cell_type":"code","metadata":{"id":"_ixYBCR8bguE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711964746032,"user_tz":-540,"elapsed":57051,"user":{"displayName":"‍황서진[재학 / 컴퓨터.전자시스템공학전공]","userId":"07764430498645957034"}},"outputId":"a9b82597-b4fc-4e87-dab1-0717a53fd80d"},"source":["!pip install transformers==2.11.0\n","!pip install tensorflow==2.2.0\n","!pip install sentencepiece==0.1.85\n","!pip install gluonnlp==0.9.1\n","!pip install mxnet==1.6.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==2.11.0\n","  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (1.25.2)\n","Collecting tokenizers==0.7.0 (from transformers==2.11.0)\n","  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (3.13.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (4.66.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2023.12.25)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (0.1.99)\n","Collecting sacremoses (from transformers==2.11.0)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2024.2.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (1.3.2)\n","Building wheels for collected packages: tokenizers\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0mFailed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.2.0\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sentencepiece==0.1.85 (from versions: 0.0.0, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.9, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.83, 0.1.86, 0.1.91, 0.1.92, 0.1.94, 0.1.95, 0.1.96, 0.1.97, 0.1.98, 0.1.99, 0.2.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for sentencepiece==0.1.85\u001b[0m\u001b[31m\n","\u001b[0mCollecting gluonnlp==0.9.1\n","  Downloading gluonnlp-0.9.1.tar.gz (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (1.25.2)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (3.0.9)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (24.0)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp310-cp310-linux_x86_64.whl size=564584 sha256=56c9f1f14a1ee10d4ad2e577191ef5efd965468abdd5bead74242206dea58ca5\n","  Stored in directory: /root/.cache/pip/wheels/fc/5b/9c/3295bb07f7c5544a96303a48988707816f44a536e8e1413922\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.9.1\n","Collecting mxnet==1.6.0\n","  Downloading mxnet-1.6.0-py2.py3-none-any.whl (68.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (1.25.2)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (2.31.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet==1.6.0)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2024.2.2)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.3\n","    Uninstalling graphviz-0.20.3:\n","      Successfully uninstalled graphviz-0.20.3\n","Successfully installed graphviz-0.8.4 mxnet-1.6.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"VPhczTnFjsG4"},"source":["## 데이터 다운로드\n","\n","* https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"]},{"cell_type":"code","metadata":{"id":"hyCKy2LtjsG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711964880697,"user_tz":-540,"elapsed":746,"user":{"displayName":"‍황서진[재학 / 컴퓨터.전자시스템공학전공]","userId":"07764430498645957034"}},"outputId":"cd6c61ba-dab9-412e-afda-51f18b64e734"},"source":["!mkdir -p gpt2\n","!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt \\\n","  -O gpt2/finetune_data.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-04-01 09:47:59--  https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 24570 (24K) [text/plain]\n","Saving to: ‘gpt2/finetune_data.txt’\n","\n","\rgpt2/finetune_data.   0%[                    ]       0  --.-KB/s               \rgpt2/finetune_data. 100%[===================>]  23.99K  --.-KB/s    in 0.002s  \n","\n","2024-04-01 09:48:00 (13.5 MB/s) - ‘gpt2/finetune_data.txt’ saved [24570/24570]\n","\n"]}]},{"cell_type":"code","source":["!pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"q7tkGRAdC69h","executionInfo":{"status":"ok","timestamp":1711964900946,"user_tz":-540,"elapsed":17804,"user":{"displayName":"‍황서진[재학 / 컴퓨터.전자시스템공학전공]","userId":"07764430498645957034"}},"outputId":"7dc6b68d-8f34-4c2f-ec72-ca992d1beb51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mxnet-mkl==1.6.0\n","  Downloading mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl (76.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy==1.23.1\n","  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (2.31.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (0.8.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.2.2)\n","Installing collected packages: numpy, mxnet-mkl\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mxnet-mkl-1.6.0 numpy-1.23.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"e2ddd61df84d4aa69e01889644e17155"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"-pGgee8pjsHB"},"source":["import os\n","import numpy as np\n","\n","import gluonnlp as nlp\n","from gluonnlp.data import SentencepieceTokenizer\n","from nltk.tokenize import sent_tokenize\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from transformers import TFGPT2LMHeadModel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROOajn6VIzgy"},"source":["## 사전 학습 모델\n","\n","* https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip"]},{"cell_type":"code","source":["!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n","!unzip -o gpt_ckpt.zip\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyNrUA7yEYoL","executionInfo":{"status":"ok","timestamp":1711965063471,"user_tz":-540,"elapsed":12605,"user":{"displayName":"‍황서진[재학 / 컴퓨터.전자시스템공학전공]","userId":"07764430498645957034"}},"outputId":"c8f3aeca-a5d4-4931-b879-b7471ec2b7df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-04-01 09:50:50--  https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:6017:18::a27d:212\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip [following]\n","--2024-04-01 09:50:51--  https://www.dropbox.com/s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com/cd/0/inline/CQNTuDPsibJ_KZNeHBUeUG8WHVePogRBj_jei6lR_nozl1cpoaNn156BRFZp04L34eNCoaNgTYW2R91vb2XmGKlGYRfEYVjhRfHZqjtUyQHzopcM9e0j_awqgt2_lFgt6KlPTfhlAwHkkr1A8xkoXBXh/file# [following]\n","--2024-04-01 09:50:51--  https://uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com/cd/0/inline/CQNTuDPsibJ_KZNeHBUeUG8WHVePogRBj_jei6lR_nozl1cpoaNn156BRFZp04L34eNCoaNgTYW2R91vb2XmGKlGYRfEYVjhRfHZqjtUyQHzopcM9e0j_awqgt2_lFgt6KlPTfhlAwHkkr1A8xkoXBXh/file\n","Resolving uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com (uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n","Connecting to uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com (uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/CQONNoAbmfXDpJuYcSE_iLaXtcyWMO8HrT3stqbruC2TNfx0z7Te6j94EWJPMj9PiUUNHcliaEcFV7wxi9CXu_aUr2InjWaTxsd3JP_C537t_qigekQ6MhIM7NQPrSUD57ZvDw-SOA0J0KfwrOlAQpLfAxi0jZH5xfAvbkYzwaGQeg5kQ-5z_A03npkgAJuKzlTa6rKY2Bqyp5_KyGbnaYrf7S9mwi-X0QAmazHAlFfqOM1uNkupVAoUiAaDanw0xmwL0Vas2L-v-99D2H7w1eQGWTAJX4i7_yTCO8L7xS2x_-8gGiee14QdwP4bjhN146gCyuF9g28zjbcTCSxGhVNGSpPnHPp2rIsJ6aiZiKfOv2W_Gq8OoxpQpsmYf3WKOhI/file [following]\n","--2024-04-01 09:50:51--  https://uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com/cd/0/inline2/CQONNoAbmfXDpJuYcSE_iLaXtcyWMO8HrT3stqbruC2TNfx0z7Te6j94EWJPMj9PiUUNHcliaEcFV7wxi9CXu_aUr2InjWaTxsd3JP_C537t_qigekQ6MhIM7NQPrSUD57ZvDw-SOA0J0KfwrOlAQpLfAxi0jZH5xfAvbkYzwaGQeg5kQ-5z_A03npkgAJuKzlTa6rKY2Bqyp5_KyGbnaYrf7S9mwi-X0QAmazHAlFfqOM1uNkupVAoUiAaDanw0xmwL0Vas2L-v-99D2H7w1eQGWTAJX4i7_yTCO8L7xS2x_-8gGiee14QdwP4bjhN146gCyuF9g28zjbcTCSxGhVNGSpPnHPp2rIsJ6aiZiKfOv2W_Gq8OoxpQpsmYf3WKOhI/file\n","Reusing existing connection to uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460908853 (440M) [application/zip]\n","Saving to: ‘gpt_ckpt.zip’\n","\n","gpt_ckpt.zip        100%[===================>] 439.56M  97.5MB/s    in 4.6s    \n","\n","2024-04-01 09:50:56 (94.8 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n","\n","Archive:  gpt_ckpt.zip\n","   creating: gpt_ckpt/\n","  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n","  inflating: gpt_ckpt/config.json    \n","  inflating: gpt_ckpt/tf_model.h5    \n"]}]},{"cell_type":"code","metadata":{"id":"1fjeaDUNjsHP"},"source":["class GPT2Model(tf.keras.Model):\n","    def __init__(self, dir_path):\n","        super(GPT2Model, self).__init__()\n","        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n","\n","    def call(self, inputs):\n","        return self.gpt2(inputs)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qlmm2I0jsHV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711965070950,"user_tz":-540,"elapsed":4693,"user":{"displayName":"‍황서진[재학 / 컴퓨터.전자시스템공학전공]","userId":"07764430498645957034"}},"outputId":"3a78cbdf-0014-48d3-e238-4a5d4b4f4869"},"source":["BASE_MODEL_PATH = './gpt_ckpt'\n","gpt_model = GPT2Model(BASE_MODEL_PATH)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n","\n","All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}]},{"cell_type":"code","metadata":{"id":"g5ilayG3jsHc"},"source":["from gluonnlp.vocab import BERTVocab\n","\n","BATCH_SIZE=16\n","NUM_EPOCHS=10\n","MAX_LEN=30\n","TOKENIZER_PATH='./gpt_ckpt/gpt2_kor_tokenizer.spiece'\n","tokenizer=SentencepieceTokenizer(TOKENIZER_PATH)\n","vocab=nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n","                                       mask_token=None,\n","                                       sep_token=None,\n","                                       cls_token=None,\n","                                       unknown_token='<unk>',\n","                                       padding_token='<pad>',\n","                                       bos_token='<s>',\n","                                       eos_token='</s>'\n","                                       )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaFAxan-jsHg"},"source":["def tf_top_k_top_p_filtering(logits, top_k = 0, top_p = 0.0, filter_value = 99999):\n","    _logits = logits.numpy()\n","    top_k = min(top_k, logits.shape[-1])\n","    if top_k > 0:\n","        indices_to_remove = logit < tf.math.top_k(logits, top_k)[0][..., -1, None]\n","        _logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        sorted_logits = tf.sort(logits, direction = 'DESCENDING')\n","        sorted_indices = tf.argsort(logits, direction = 'DESCENDING')\n","        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis = -1), axis = -1)\n","\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis = 0)\n","        indeices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n","\n","        _logits[indices_to_remove] = filter_value\n","\n","    return tf.constant([_logits])\n","\n","def generate_sentence(seed_word, model, max_step = 100, greedy = False, top_k = 0, top_p = 0.):\n","    sentence = seed_word\n","    toked = tokenizer(sentence)\n","\n","    for _ in range(max_step):\n","        input_ids = tf.constant([vocab[vocab.bos_token], ]+ vocab[toked])[None, :]\n","        outputs = model(input_ids)[:, -1, :]\n","        if greedy:\n","            gen = vocab.to_tokens(tf.argmax(outputs, axis = -1).numpy().tolist()[0])\n","        else:\n","            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k = top_k, top_p = top_p)\n","            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n","        if gen == '<\\s>':\n","            break\n","        sentence += gen.replace('-', ' ')\n","        toked = tokenizer(sentence)\n","\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6bhahzWjsHl","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1711965164989,"user_tz":-540,"elapsed":279,"user":{"displayName":"‍황서진[재학 / 컴퓨터.전자시스템공학전공]","userId":"07764430498645957034"}},"outputId":"b163d6c7-08a1-48f1-bb0c-2feb6aa8b3a5"},"source":["generate_sentence('일부', gpt_model, greedy = True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d3ea760be3b5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'일부'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-fb2285e46b93>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(seed_word, model, max_step, greedy, top_k, top_p)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtoked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         return self._processor.SampleEncodeAsPieces(sample, self._nbest,\n\u001b[0m\u001b[1;32m    562\u001b[0m                                                     self._alpha)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mSampleEncodeAsPieces\u001b[0;34m(self, input, nbest_size, alpha, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       return self.Encode(input=input, nbest_size=nbest_size, alpha=alpha,\n\u001b[0m\u001b[1;32m    562\u001b[0m                          out_type=str, enable_sampling=True, **kwargs)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mEncode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    501\u001b[0m       if enable_sampling == True and (nbest_size is None or nbest_size == 0 or\n\u001b[1;32m    502\u001b[0m                                       nbest_size == 1 or alpha is None):\n\u001b[0;32m--> 503\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;34m'When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;34m'and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations."]}]},{"cell_type":"code","metadata":{"id":"GW5jmfiejsHr","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1711965249478,"user_tz":-540,"elapsed":337,"user":{"displayName":"‍황서진[재학 / 컴퓨터.전자시스템공학전공]","userId":"07764430498645957034"}},"outputId":"66a78d46-3cd8-4ad3-fd82-a5fa924ce92a"},"source":["generate_sentence('일부', gpt_model, top_k = 0, top_p = 0.95)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-1a352bec2ba9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'일부'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-fb2285e46b93>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(seed_word, model, max_step, greedy, top_k, top_p)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtoked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         return self._processor.SampleEncodeAsPieces(sample, self._nbest,\n\u001b[0m\u001b[1;32m    562\u001b[0m                                                     self._alpha)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mSampleEncodeAsPieces\u001b[0;34m(self, input, nbest_size, alpha, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       return self.Encode(input=input, nbest_size=nbest_size, alpha=alpha,\n\u001b[0m\u001b[1;32m    562\u001b[0m                          out_type=str, enable_sampling=True, **kwargs)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mEncode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    501\u001b[0m       if enable_sampling == True and (nbest_size is None or nbest_size == 0 or\n\u001b[1;32m    502\u001b[0m                                       nbest_size == 1 or alpha is None):\n\u001b[0;32m--> 503\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;34m'When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;34m'and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations."]}]},{"cell_type":"markdown","metadata":{"id":"-BZEEq4mIMhr"},"source":["# GPT2 네이버 영화 리뷰 분류"]},{"cell_type":"markdown","metadata":{"id":"ZTXFkRQYxGa0"},"source":["## 데이터 다운로드"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ijkw_0U2xGa-"},"source":["import re\n","import urllib.request\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-white')\n","\n","from transformers import TFGPT2Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNs8XHaUxGbQ"},"source":["tf.random.set_seed(111)\n","np.random.seed(111)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcbcRQKwxGbW"},"source":["## 데이터 준비"]},{"cell_type":"code","metadata":{"id":"wpABh-81xGbW"},"source":["BATCH_SIZE = 32\n","NUM_EPOCHS = 3\n","VALID_SPLIT = 0.1\n","SENT_MAX_LEN = 39"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqPVUEwjxGbb"},"source":["TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n","\n","tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n","                                               mask_token = None,\n","                                               sep_token = '',\n","                                               cls_toeken = None,\n","                                               unknown_token = '',\n","                                               padding_token = '',\n","                                               bos_token = '',\n","                                               eos_token = '<\\s>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0I6dM15ym7uK"},"source":["* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n","* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"]},{"cell_type":"code","metadata":{"id":"IetCxzkbxGbf"},"source":["train_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\")\n","test_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\")\n","\n","train_data = pd.read_table(train_file)\n","test_data = pd.read_table(test_file)\n","\n","train_data = train_data.dropna()\n","test_data = test_data.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_ZCDWgskiRp"},"source":["train_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVnAFFU-kiny"},"source":["test_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lF8f3VcJxGbj"},"source":["def clean_text(text):\n","    text_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", text)\n","\n","    return text_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuAoVmTGxGbo"},"source":["train_data_sents = []\n","train_data_labels = []\n","\n","for train_sent, train_label in train_data[['document', 'label']].values:\n","    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n","\n","    tokens = [vocab[vocab.bos_token]]\n","    tokens += pad_sequences([train_tokenized_text],\n","                            SENT_MAX_LEN,\n","                            value = vocab[vocab.padding_token],\n","                            padding = 'post').tolist()[0]\n","    tokens += [vocab[vocab.eos_token]]\n","\n","    train_data_sents.append(tokens)\n","    train_data_labels.append(train_label)\n","\n","train_data_sents = np.array(train_data_sents, dtype = np.int64)\n","train_data_labels = np.array(train_data_labels, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4w_U2EMQxGbs"},"source":["## 모델 학습"]},{"cell_type":"code","metadata":{"id":"5JYb6XjgxGbu"},"source":["class TFGPT2Classifier(tf.keras.Model):\n","    def __init__(self, dir_path, num_class):\n","        super(TFGPT2Classifier, self).__init__()\n","\n","        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n","        self.num_class = num_class\n","\n","        self.dropout = tf.keras.layers.Dropout(self.gpt2. config.summary_first_dropout)\n","        self.classifier = tf.keras.layers.Dense(self.num_class,\n","                                                kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = self.gpt2.config.initializer_range),\n","                                                name = 'classifier')\n","\n","    def call(self, inputs):\n","        outputs = self.gpt2(inputs)\n","        pooled_output = outputs[0][:, -1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self. classifier(pooled_output)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oUfrW5TxGby"},"source":["BASE_MODEL_PATH = './gpt_ckpt'\n","cls_model = TFGPT2Classifier(dir_path = BASE_MODEL_PATH, num_class = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OsxKKImxGb1"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate = 6.25e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","cls_model.compile(optimizer = optimizer, loss = loss, metrics = [metric])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRF8D388xGb5"},"source":["model_name = 'tf2_gpt2_naver_movie'\n","\n","es_callback = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0001, patience = 2)\n","\n","checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} directory already exists\\n\".format(checkpoint_dir))\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok = True)\n","    print(\"{} directory already complete\\n\".format(checkpoint_dir))\n","\n","cp_callback = ModelCheckpoint(checkpoint_path,\n","                              monitor = 'val_accuracy',\n","                              verbose = 1,\n","                              save_best_only = True,\n","                              save_weights_only = True)\n","\n","history = cls_model.fit(train_data_sents, train_data_labels,\n","                        epochs = NUM_EPOCHS,\n","                        batch_size = BATCH_SIZE,\n","                        validation_split = VALID_SPLIT,\n","                        callbacks = [es_callback, cp_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGj4h0l3xGb9"},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'], '')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['Loss', 'Validation Loss'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7x-FC6BDxGcB"},"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'], '')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['Accuracy', 'Validation Accuracy'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKJx63kSxGcF"},"source":["## 모델 평가"]},{"cell_type":"code","metadata":{"id":"VywcseLrxGcH"},"source":["test_data_sents = []\n","test_data_labels = []\n","\n","for test_sent, test_label in test_data[['document', 'label']].values:\n","    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\n","\n","    tokens = [vocab[vocab.bos_token]]\n","    tokens += pad_sequences([test_tokenized_text],\n","                            SENT_MAX_LEN,\n","                            value = vocab[vocab.padding_token],\n","                            padding = 'post').tolist()[0]\n","    tokens += [vocab[vocab.eos_token]]\n","\n","    test_data_sents.append(tokens)\n","    test_data_labels.append(test_label)\n","\n","test_data_sents = np.array(test_data_sents, dtype = np.int64)\n","test_data_labels = np.array(test_data_labels, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wj3dRljzxGcP"},"source":["cls_model.load_weights(checkpoint_path)\n","cls_model.evaluate(test_data_sents, test_data_labels, batch_size = 1024)"],"execution_count":null,"outputs":[]}]}