{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBjBNQX8kQfh"
   },
   "source": [
    "# GPT(Generative Pre-trained Transformer) 2\n",
    "\n",
    "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKeqNH_dkTmT"
   },
   "source": [
    "* OpenAI에서 GPT 모델 제안\n",
    "* 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝\n",
    "* BERT와 마찬가지로 트랜스포머 모델이지만, BERT는 트랜스포머의 인코더 구조만 사용하고, GPT는 트랜스포머의 디코더 구조(순방향 어텐션)만 사용\n",
    "\n",
    "* GPT2는 GPT1에서 개선되어 레이어 정규화가 부분 블록의 입력쪽에서 사용되고, 셀프 어텐션 이후에 레이어 정규화 적용\n",
    "* GPT2는 GPT1에 비교해 크기가 매우 커진 향상된 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDCr0YqjbfLJ"
   },
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57051,
     "status": "ok",
     "timestamp": 1711964746032,
     "user": {
      "displayName": "‍황서진[재학 / 컴퓨터.전자시스템공학전공]",
      "userId": "07764430498645957034"
     },
     "user_tz": -540
    },
    "id": "_ixYBCR8bguE",
    "outputId": "a9b82597-b4fc-4e87-dab1-0717a53fd80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.11.0\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (1.25.2)\n",
      "Collecting tokenizers==0.7.0 (from transformers==2.11.0)\n",
      "  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (24.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (3.13.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (4.66.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2023.12.25)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (0.1.99)\n",
      "Collecting sacremoses (from transformers==2.11.0)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2024.2.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (1.3.2)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.2.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sentencepiece==0.1.85 (from versions: 0.0.0, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.9, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.83, 0.1.86, 0.1.91, 0.1.92, 0.1.94, 0.1.95, 0.1.96, 0.1.97, 0.1.98, 0.1.99, 0.2.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for sentencepiece==0.1.85\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting gluonnlp==0.9.1\n",
      "  Downloading gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (1.25.2)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (3.0.9)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (24.0)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp310-cp310-linux_x86_64.whl size=564584 sha256=56c9f1f14a1ee10d4ad2e577191ef5efd965468abdd5bead74242206dea58ca5\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/5b/9c/3295bb07f7c5544a96303a48988707816f44a536e8e1413922\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: gluonnlp\n",
      "Successfully installed gluonnlp-0.9.1\n",
      "Collecting mxnet==1.6.0\n",
      "  Downloading mxnet-1.6.0-py2.py3-none-any.whl (68.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (2.31.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet==1.6.0)\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2024.2.2)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.20.3\n",
      "    Uninstalling graphviz-0.20.3:\n",
      "      Successfully uninstalled graphviz-0.20.3\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.11.0\n",
    "!pip install tensorflow==2.2.0\n",
    "!pip install sentencepiece==0.1.85\n",
    "!pip install gluonnlp==0.9.1\n",
    "!pip install mxnet==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPhczTnFjsG4"
   },
   "source": [
    "## 데이터 다운로드\n",
    "\n",
    "* https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1711964880697,
     "user": {
      "displayName": "‍황서진[재학 / 컴퓨터.전자시스템공학전공]",
      "userId": "07764430498645957034"
     },
     "user_tz": -540
    },
    "id": "hyCKy2LtjsG7",
    "outputId": "cd6c61ba-dab9-412e-afda-51f18b64e734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-01 09:47:59--  https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24570 (24K) [text/plain]\n",
      "Saving to: ‘gpt2/finetune_data.txt’\n",
      "\n",
      "\r",
      "gpt2/finetune_data.   0%[                    ]       0  --.-KB/s               \r",
      "gpt2/finetune_data. 100%[===================>]  23.99K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-04-01 09:48:00 (13.5 MB/s) - ‘gpt2/finetune_data.txt’ saved [24570/24570]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p gpt2\n",
    "!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt \\\n",
    "  -O gpt2/finetune_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "executionInfo": {
     "elapsed": 17804,
     "status": "ok",
     "timestamp": 1711964900946,
     "user": {
      "displayName": "‍황서진[재학 / 컴퓨터.전자시스템공학전공]",
      "userId": "07764430498645957034"
     },
     "user_tz": -540
    },
    "id": "q7tkGRAdC69h",
    "outputId": "7dc6b68d-8f34-4c2f-ec72-ca992d1beb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-mkl==1.6.0\n",
      "  Downloading mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl (76.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.23.1\n",
      "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (2.31.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (0.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.2.2)\n",
      "Installing collected packages: numpy, mxnet-mkl\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n",
      "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mxnet-mkl-1.6.0 numpy-1.23.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e2ddd61df84d4aa69e01889644e17155",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pGgee8pjsHB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROOajn6VIzgy"
   },
   "source": [
    "## 사전 학습 모델\n",
    "\n",
    "* https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12605,
     "status": "ok",
     "timestamp": 1711965063471,
     "user": {
      "displayName": "‍황서진[재학 / 컴퓨터.전자시스템공학전공]",
      "userId": "07764430498645957034"
     },
     "user_tz": -540
    },
    "id": "QyNrUA7yEYoL",
    "outputId": "c8f3aeca-a5d4-4931-b879-b7471ec2b7df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-01 09:50:50--  https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:6017:18::a27d:212\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip [following]\n",
      "--2024-04-01 09:50:51--  https://www.dropbox.com/s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com/cd/0/inline/CQNTuDPsibJ_KZNeHBUeUG8WHVePogRBj_jei6lR_nozl1cpoaNn156BRFZp04L34eNCoaNgTYW2R91vb2XmGKlGYRfEYVjhRfHZqjtUyQHzopcM9e0j_awqgt2_lFgt6KlPTfhlAwHkkr1A8xkoXBXh/file# [following]\n",
      "--2024-04-01 09:50:51--  https://uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com/cd/0/inline/CQNTuDPsibJ_KZNeHBUeUG8WHVePogRBj_jei6lR_nozl1cpoaNn156BRFZp04L34eNCoaNgTYW2R91vb2XmGKlGYRfEYVjhRfHZqjtUyQHzopcM9e0j_awqgt2_lFgt6KlPTfhlAwHkkr1A8xkoXBXh/file\n",
      "Resolving uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com (uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
      "Connecting to uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com (uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/CQONNoAbmfXDpJuYcSE_iLaXtcyWMO8HrT3stqbruC2TNfx0z7Te6j94EWJPMj9PiUUNHcliaEcFV7wxi9CXu_aUr2InjWaTxsd3JP_C537t_qigekQ6MhIM7NQPrSUD57ZvDw-SOA0J0KfwrOlAQpLfAxi0jZH5xfAvbkYzwaGQeg5kQ-5z_A03npkgAJuKzlTa6rKY2Bqyp5_KyGbnaYrf7S9mwi-X0QAmazHAlFfqOM1uNkupVAoUiAaDanw0xmwL0Vas2L-v-99D2H7w1eQGWTAJX4i7_yTCO8L7xS2x_-8gGiee14QdwP4bjhN146gCyuF9g28zjbcTCSxGhVNGSpPnHPp2rIsJ6aiZiKfOv2W_Gq8OoxpQpsmYf3WKOhI/file [following]\n",
      "--2024-04-01 09:50:51--  https://uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com/cd/0/inline2/CQONNoAbmfXDpJuYcSE_iLaXtcyWMO8HrT3stqbruC2TNfx0z7Te6j94EWJPMj9PiUUNHcliaEcFV7wxi9CXu_aUr2InjWaTxsd3JP_C537t_qigekQ6MhIM7NQPrSUD57ZvDw-SOA0J0KfwrOlAQpLfAxi0jZH5xfAvbkYzwaGQeg5kQ-5z_A03npkgAJuKzlTa6rKY2Bqyp5_KyGbnaYrf7S9mwi-X0QAmazHAlFfqOM1uNkupVAoUiAaDanw0xmwL0Vas2L-v-99D2H7w1eQGWTAJX4i7_yTCO8L7xS2x_-8gGiee14QdwP4bjhN146gCyuF9g28zjbcTCSxGhVNGSpPnHPp2rIsJ6aiZiKfOv2W_Gq8OoxpQpsmYf3WKOhI/file\n",
      "Reusing existing connection to uc8b001c4f804555d218d1036b34.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 460908853 (440M) [application/zip]\n",
      "Saving to: ‘gpt_ckpt.zip’\n",
      "\n",
      "gpt_ckpt.zip        100%[===================>] 439.56M  97.5MB/s    in 4.6s    \n",
      "\n",
      "2024-04-01 09:50:56 (94.8 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n",
      "\n",
      "Archive:  gpt_ckpt.zip\n",
      "   creating: gpt_ckpt/\n",
      "  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n",
      "  inflating: gpt_ckpt/config.json    \n",
      "  inflating: gpt_ckpt/tf_model.h5    \n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
    "!unzip -o gpt_ckpt.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fjeaDUNjsHP"
   },
   "outputs": [],
   "source": [
    "class GPT2Model(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.gpt2(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4693,
     "status": "ok",
     "timestamp": 1711965070950,
     "user": {
      "displayName": "‍황서진[재학 / 컴퓨터.전자시스템공학전공]",
      "userId": "07764430498645957034"
     },
     "user_tz": -540
    },
    "id": "9qlmm2I0jsHV",
    "outputId": "3a78cbdf-0014-48d3-e238-4a5d4b4f4869"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "gpt_model = GPT2Model(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5ilayG3jsHc"
   },
   "outputs": [],
   "source": [
    "from gluonnlp.vocab import BERTVocab\n",
    "\n",
    "BATCH_SIZE=16\n",
    "NUM_EPOCHS=10\n",
    "MAX_LEN=30\n",
    "TOKENIZER_PATH='./gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "tokenizer=SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab=nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                       mask_token=None,\n",
    "                                       sep_token=None,\n",
    "                                       cls_token=None,\n",
    "                                       unknown_token='<unk>',\n",
    "                                       padding_token='<pad>',\n",
    "                                       bos_token='<s>',\n",
    "                                       eos_token='</s>'\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaFAxan-jsHg"
   },
   "outputs": [],
   "source": [
    "def tf_top_k_top_p_filtering(logits, top_k = 0, top_p = 0.0, filter_value = 99999):\n",
    "    _logits = logits.numpy()\n",
    "    top_k = min(top_k, logits.shape[-1])\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logit < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits = tf.sort(logits, direction = 'DESCENDING')\n",
    "        sorted_indices = tf.argsort(logits, direction = 'DESCENDING')\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis = -1), axis = -1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis = 0)\n",
    "        indeices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
    "\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return tf.constant([_logits])\n",
    "\n",
    "def generate_sentence(seed_word, model, max_step = 100, greedy = False, top_k = 0, top_p = 0.):\n",
    "    sentence = seed_word\n",
    "    toked = tokenizer(sentence)\n",
    "\n",
    "    for _ in range(max_step):\n",
    "        input_ids = tf.constant([vocab[vocab.bos_token], ]+ vocab[toked])[None, :]\n",
    "        outputs = model(input_ids)[:, -1, :]\n",
    "        if greedy:\n",
    "            gen = vocab.to_tokens(tf.argmax(outputs, axis = -1).numpy().tolist()[0])\n",
    "        else:\n",
    "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k = top_k, top_p = top_p)\n",
    "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
    "        if gen == '<\\s>':\n",
    "            break\n",
    "        sentence += gen.replace('-', ' ')\n",
    "        toked = tokenizer(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "error",
     "timestamp": 1711965164989,
     "user": {
      "displayName": "‍황서진[재학 / 컴퓨터.전자시스템공학전공]",
      "userId": "07764430498645957034"
     },
     "user_tz": -540
    },
    "id": "q6bhahzWjsHl",
    "outputId": "b163d6c7-08a1-48f1-bb0c-2feb6aa8b3a5"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d3ea760be3b5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'일부'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-fb2285e46b93>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(seed_word, model, max_step, greedy, top_k, top_p)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtoked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         return self._processor.SampleEncodeAsPieces(sample, self._nbest,\n\u001b[0m\u001b[1;32m    562\u001b[0m                                                     self._alpha)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mSampleEncodeAsPieces\u001b[0;34m(self, input, nbest_size, alpha, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       return self.Encode(input=input, nbest_size=nbest_size, alpha=alpha,\n\u001b[0m\u001b[1;32m    562\u001b[0m                          out_type=str, enable_sampling=True, **kwargs)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mEncode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    501\u001b[0m       if enable_sampling == True and (nbest_size is None or nbest_size == 0 or\n\u001b[1;32m    502\u001b[0m                                       nbest_size == 1 or alpha is None):\n\u001b[0;32m--> 503\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;34m'When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;34m'and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations."
     ]
    }
   ],
   "source": [
    "generate_sentence('일부', gpt_model, greedy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "error",
     "timestamp": 1711965249478,
     "user": {
      "displayName": "‍황서진[재학 / 컴퓨터.전자시스템공학전공]",
      "userId": "07764430498645957034"
     },
     "user_tz": -540
    },
    "id": "GW5jmfiejsHr",
    "outputId": "66a78d46-3cd8-4ad3-fd82-a5fa924ce92a"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1a352bec2ba9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'일부'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-fb2285e46b93>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(seed_word, model, max_step, greedy, top_k, top_p)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtoked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         return self._processor.SampleEncodeAsPieces(sample, self._nbest,\n\u001b[0m\u001b[1;32m    562\u001b[0m                                                     self._alpha)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mSampleEncodeAsPieces\u001b[0;34m(self, input, nbest_size, alpha, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       return self.Encode(input=input, nbest_size=nbest_size, alpha=alpha,\n\u001b[0m\u001b[1;32m    562\u001b[0m                          out_type=str, enable_sampling=True, **kwargs)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mEncode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    501\u001b[0m       if enable_sampling == True and (nbest_size is None or nbest_size == 0 or\n\u001b[1;32m    502\u001b[0m                                       nbest_size == 1 or alpha is None):\n\u001b[0;32m--> 503\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;34m'When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;34m'and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations."
     ]
    }
   ],
   "source": [
    "generate_sentence('일부', gpt_model, top_k = 0, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BZEEq4mIMhr"
   },
   "source": [
    "# GPT2 네이버 영화 리뷰 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTXFkRQYxGa0"
   },
   "source": [
    "## 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijkw_0U2xGa-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "from transformers import TFGPT2Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNs8XHaUxGbQ"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(111)\n",
    "np.random.seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcbcRQKwxGbW"
   },
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpABh-81xGbW"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.1\n",
    "SENT_MAX_LEN = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqPVUEwjxGbb"
   },
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                               mask_token = None,\n",
    "                                               sep_token = '',\n",
    "                                               cls_toeken = None,\n",
    "                                               unknown_token = '',\n",
    "                                               padding_token = '',\n",
    "                                               bos_token = '',\n",
    "                                               eos_token = '<\\s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I6dM15ym7uK"
   },
   "source": [
    "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IetCxzkbxGbf"
   },
   "outputs": [],
   "source": [
    "train_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\")\n",
    "test_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_table(train_file)\n",
    "test_data = pd.read_table(test_file)\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_ZCDWgskiRp"
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVnAFFU-kiny"
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lF8f3VcJxGbj"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", text)\n",
    "\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuAoVmTGxGbo"
   },
   "outputs": [],
   "source": [
    "train_data_sents = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in train_data[['document', 'label']].values:\n",
    "    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n",
    "\n",
    "    tokens = [vocab[vocab.bos_token]]\n",
    "    tokens += pad_sequences([train_tokenized_text],\n",
    "                            SENT_MAX_LEN,\n",
    "                            value = vocab[vocab.padding_token],\n",
    "                            padding = 'post').tolist()[0]\n",
    "    tokens += [vocab[vocab.eos_token]]\n",
    "\n",
    "    train_data_sents.append(tokens)\n",
    "    train_data_labels.append(train_label)\n",
    "\n",
    "train_data_sents = np.array(train_data_sents, dtype = np.int64)\n",
    "train_data_labels = np.array(train_data_labels, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w_U2EMQxGbs"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JYb6XjgxGbu"
   },
   "outputs": [],
   "source": [
    "class TFGPT2Classifier(tf.keras.Model):\n",
    "    def __init__(self, dir_path, num_class):\n",
    "        super(TFGPT2Classifier, self).__init__()\n",
    "\n",
    "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
    "        self.num_class = num_class\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(self.gpt2. config.summary_first_dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(self.num_class,\n",
    "                                                kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = self.gpt2.config.initializer_range),\n",
    "                                                name = 'classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.gpt2(inputs)\n",
    "        pooled_output = outputs[0][:, -1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self. classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oUfrW5TxGby"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "cls_model = TFGPT2Classifier(dir_path = BASE_MODEL_PATH, num_class = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OsxKKImxGb1"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 6.25e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer = optimizer, loss = loss, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRF8D388xGb5"
   },
   "outputs": [],
   "source": [
    "model_name = 'tf2_gpt2_naver_movie'\n",
    "\n",
    "es_callback = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0001, patience = 2)\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} directory already exists\\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "    print(\"{} directory already complete\\n\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                              monitor = 'val_accuracy',\n",
    "                              verbose = 1,\n",
    "                              save_best_only = True,\n",
    "                              save_weights_only = True)\n",
    "\n",
    "history = cls_model.fit(train_data_sents, train_data_labels,\n",
    "                        epochs = NUM_EPOCHS,\n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        validation_split = VALID_SPLIT,\n",
    "                        callbacks = [es_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGj4h0l3xGb9"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'], '')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Loss', 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x-FC6BDxGcB"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'], '')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Accuracy', 'Validation Accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKJx63kSxGcF"
   },
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VywcseLrxGcH"
   },
   "outputs": [],
   "source": [
    "test_data_sents = []\n",
    "test_data_labels = []\n",
    "\n",
    "for test_sent, test_label in test_data[['document', 'label']].values:\n",
    "    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\n",
    "\n",
    "    tokens = [vocab[vocab.bos_token]]\n",
    "    tokens += pad_sequences([test_tokenized_text],\n",
    "                            SENT_MAX_LEN,\n",
    "                            value = vocab[vocab.padding_token],\n",
    "                            padding = 'post').tolist()[0]\n",
    "    tokens += [vocab[vocab.eos_token]]\n",
    "\n",
    "    test_data_sents.append(tokens)\n",
    "    test_data_labels.append(test_label)\n",
    "\n",
    "test_data_sents = np.array(test_data_sents, dtype = np.int64)\n",
    "test_data_labels = np.array(test_data_labels, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj3dRljzxGcP"
   },
   "outputs": [],
   "source": [
    "cls_model.load_weights(checkpoint_path)\n",
    "cls_model.evaluate(test_data_sents, test_data_labels, batch_size = 1024)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1cyRyTv3BZHahrjmbvnNW0tZkFDT0lM-G",
     "timestamp": 1711894433581
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
